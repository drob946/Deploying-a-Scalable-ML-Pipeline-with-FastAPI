{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c51150a-d084-43ed-8e1a-970ee1437774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age         workclass   fnlgt  education  education-num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country salary  \n",
      "0          2174             0              40  United-States  <=50K  \n",
      "1             0             0              13  United-States  <=50K  \n",
      "2             0             0              40  United-States  <=50K  \n",
      "3             0             0              40  United-States  <=50K  \n",
      "4             0             0              40           Cuba  <=50K  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlgt           32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  salary          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n",
      "None\n",
      "                age         fnlgt  education-num  capital-gain  capital-loss  \\\n",
      "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
      "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
      "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
      "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours-per-week  \n",
      "count    32561.000000  \n",
      "mean        40.437456  \n",
      "std         12.347429  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"data/census.csv\")\n",
    "\n",
    "# Inspect the data\n",
    "print(data.head())  # First few rows\n",
    "print(data.info())  # Data types and non-null values\n",
    "print(data.describe())  # Statistical summary for numeric column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c61ebd5-c186-4eff-8b32-d7f2c1609465",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"workclass\", \"education\", \"marital-status\", \"occupation\",\n",
    "    \"relationship\", \"race\", \"sex\", \"native-country\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9f4184-0dec-4638-802b-26a599c3da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"salary\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6541f34-f615-42b4-912c-c6eab3c0c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Features Shape: (32561, 108)\n",
      "Processed Labels Shape: (32561,)\n",
      "Sample Processed Features (First 5 Rows):\n",
      " [[3.90000e+01 7.75160e+04 1.30000e+01 2.17400e+03 0.00000e+00 4.00000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [5.00000e+01 8.33110e+04 1.30000e+01 0.00000e+00 0.00000e+00 1.30000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [3.80000e+01 2.15646e+05 9.00000e+00 0.00000e+00 0.00000e+00 4.00000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [5.30000e+01 2.34721e+05 7.00000e+00 0.00000e+00 0.00000e+00 4.00000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [2.80000e+01 3.38409e+05 1.30000e+01 0.00000e+00 0.00000e+00 4.00000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]]\n",
      "Sample Processed Labels (First 5 Rows):\n",
      " [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from ml.data import process_data\n",
    "\n",
    "# Process the data for training\n",
    "X, y, encoder, lb = process_data(\n",
    "    data,\n",
    "    categorical_features=categorical_features,\n",
    "    label=label_column,\n",
    "    training=True\n",
    ")\n",
    "\n",
    "# Inspect the processed features and labels\n",
    "print(\"Processed Features Shape:\", X.shape)\n",
    "print(\"Processed Labels Shape:\", y.shape)\n",
    "print(\"Sample Processed Features (First 5 Rows):\\n\", X[:5])\n",
    "print(\"Sample Processed Labels (First 5 Rows):\\n\", y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820ce931-ff65-4723-9c78-4a85978b07a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of one-hot encoded features: 102\n"
     ]
    }
   ],
   "source": [
    "num_categories = sum(len(encoder.categories_[i]) for i in range(len(encoder.categories_)))\n",
    "print(f\"Number of one-hot encoded features: {num_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef74d94-4d16-41fe-8731-926251d61e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Features Shape: (32561, 108)\n",
      "Sample Inference Features (First 5 Rows):\n",
      " [[3.90000e+01 7.75160e+04 1.30000e+01 2.17400e+03 0.00000e+00 4.00000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [5.00000e+01 8.33110e+04 1.30000e+01 0.00000e+00 0.00000e+00 1.30000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [3.80000e+01 2.15646e+05 9.00000e+00 0.00000e+00 0.00000e+00 4.00000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [5.30000e+01 2.34721e+05 7.00000e+00 0.00000e+00 0.00000e+00 4.00000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [2.80000e+01 3.38409e+05 1.30000e+01 0.00000e+00 0.00000e+00 4.00000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "X_inference, y_inference, _, _ = process_data(\n",
    "    data,\n",
    "    categorical_features=categorical_features,\n",
    "    label=label_column,\n",
    "    training=False,\n",
    "    encoder=encoder,\n",
    "    lb=lb\n",
    ")\n",
    "print(\"Inference Features Shape:\", X_inference.shape)\n",
    "print(\"Sample Inference Features (First 5 Rows):\\n\", X_inference[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90ae6ff-257d-4d3d-99e7-90e45bdbddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical column workclass: 9 unique values\n",
      "Categorical column education: 16 unique values\n",
      "Categorical column marital-status: 7 unique values\n",
      "Categorical column occupation: 15 unique values\n",
      "Categorical column relationship: 6 unique values\n",
      "Categorical column race: 5 unique values\n",
      "Categorical column sex: 2 unique values\n",
      "Categorical column native-country: 42 unique values\n"
     ]
    }
   ],
   "source": [
    "for i, category in enumerate(encoder.categories_):\n",
    "    print(f\"Categorical column {categorical_features[i]}: {len(category)} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c085af6c-55ef-4f20-8e51-94cefcfedb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Continuous Features (First 5 Rows):\n",
      " [[ 0.03067056 -1.06361075  1.13473876  0.1484529  -0.21665953 -0.03542945]\n",
      " [ 0.83710898 -1.008707    1.13473876 -0.14592048 -0.21665953 -2.22215312]\n",
      " [-0.04264203  0.2450785  -0.42005962 -0.14592048 -0.21665953 -0.03542945]\n",
      " [ 1.05704673  0.42580136 -1.19745882 -0.14592048 -0.21665953 -0.03542945]\n",
      " [-0.77576787  1.40817572  1.13473876 -0.14592048 -0.21665953 -0.03542945]]\n",
      "Combined Features Shape: (32561, 108)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# Define numerical columns\n",
    "numerical_columns = [\"age\", \"fnlgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "# Extract continuous features from the original dataset\n",
    "X_continuous = data[numerical_columns].values\n",
    "\n",
    "# Apply scaling\n",
    "scaler = StandardScaler()\n",
    "X_continuous_scaled = scaler.fit_transform(X_continuous)\n",
    "\n",
    "# Combine with previously processed categorical features\n",
    "X_combined = np.concatenate([X_continuous_scaled, X[:, len(numerical_columns):]], axis=1)\n",
    "\n",
    "# Verify the result\n",
    "print(\"Scaled Continuous Features (First 5 Rows):\\n\", X_continuous_scaled[:5])\n",
    "print(\"Combined Features Shape:\", X_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7817827-9b82-4da5-bf56-a733cc5ba7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def process_data(\n",
    "    X, categorical_features=[], label=None, training=True, encoder=None, lb=None, scaler=None\n",
    "):\n",
    "    # Split label from features\n",
    "    if label is not None:\n",
    "        y = X[label]\n",
    "        X = X.drop([label], axis=1)\n",
    "    else:\n",
    "        y = np.array([])\n",
    "\n",
    "    # Separate continuous and categorical features\n",
    "    X_categorical = X[categorical_features].values\n",
    "    X_continuous = X.drop(columns=categorical_features).values\n",
    "\n",
    "    if training:\n",
    "        # Initialize encoders and scaler\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "        lb = LabelBinarizer()\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit and transform categorical and continuous features\n",
    "        X_categorical = encoder.fit_transform(X_categorical)\n",
    "        X_continuous = scaler.fit_transform(X_continuous)\n",
    "        y = lb.fit_transform(y.values).ravel()\n",
    "    else:\n",
    "        # Transform using existing encoders and scaler\n",
    "        X_categorical = encoder.transform(X_categorical)\n",
    "        X_continuous = scaler.transform(X_continuous)\n",
    "        try:\n",
    "            y = lb.transform(y.values).ravel()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    # Combine scaled continuous and one-hot encoded categorical features\n",
    "    X = np.concatenate([X_continuous, X_categorical], axis=1)\n",
    "\n",
    "    return X, y, encoder, lb, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56779b0a-99dc-420a-8f96-9d1eb2c1c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (26048, 108)\n",
      "Testing Features Shape: (6513, 108)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Testing Features Shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d76a435-7962-423d-af82-e67f991d2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.model import (\n",
    "    train_model,\n",
    "    inference,\n",
    "    save_model,\n",
    "    load_model,\n",
    "    performance_on_categorical_slice,\n",
    "    compute_model_metrics\n",
    ")\n",
    "from ml.data import process_data\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9905d279-8abe-403b-9d8f-0fe363706ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"data/census.csv\")\n",
    "\n",
    "# Define categorical features and label column\n",
    "categorical_features = [\n",
    "    \"workclass\", \"education\", \"marital-status\", \"occupation\",\n",
    "    \"relationship\", \"race\", \"sex\", \"native-country\"\n",
    "]\n",
    "label_column = \"salary\"\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, encoder, lb = process_data(\n",
    "    data, categorical_features=categorical_features, label=label_column, training=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8881abae-145d-46ff-bfef-77f1fb3a5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ee4879-55f1-4851-bd05-32bf03431544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train_model(X_train, y_train)\n",
    "print(\"Model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53e44ce5-3564-4073-b271-d5bdebb68ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "save_model(model, \"model.pkl\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model(\"model.pkl\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f820edc-9922-46da-8c5c-5ced5232767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.74, Recall: 0.64, F1-Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "y_preds = inference(loaded_model, X_test)\n",
    "\n",
    "# Compute metrics\n",
    "precision, recall, fbeta = compute_model_metrics(y_test, y_preds)\n",
    "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {fbeta:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57331a56-e3eb-4068-968c-67a00254f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for education = Bachelors\n",
      "Precision: 0.95, Recall: 0.95, F1-Score: 0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9510647938377889),\n",
       " np.float64(0.945069788383611),\n",
       " np.float64(0.9480578139114725))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_on_categorical_slice(\n",
    "    data=data,\n",
    "    column_name=\"education\",\n",
    "    slice_value=\"Bachelors\",\n",
    "    categorical_features=categorical_features,\n",
    "    label=label_column,\n",
    "    encoder=encoder,\n",
    "    lb=lb,\n",
    "    model=loaded_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684fe21-b04a-4f61-a7e1-68aed82a63e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e031999-d018-4974-80ff-ebdd617a99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from ml import model  # Import the module again (if not already imported)\n",
    "importlib.reload(model)\n",
    "\n",
    "# Re-import the functions if needed\n",
    "from ml.model import performance_on_categorical_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b5ff7-7c4f-468b-8a10-7f4e97d59c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
